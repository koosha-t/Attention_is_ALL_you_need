# Complete BERT Tutorial (in progress)

The appearance of the BERT (*Bidiractional Encoder Representaions from Transformers*) model in 2018 has certainly been an infelection point in the NLP world. "BERT models have demonstrated a very sophisticated knowledge of language, achieving human-level performance on __certain tasks__" [[1]](#1). BERT is a pre-trained model (trained by Google on a huge corpus), and can be fine-tuned and repurposed for different NLP tasks.



## References
<a id="1">[1]</a> 
Chris McCormick. The Inner Workings of BERT (2020)
